---
title: "Réalisation d'une ACM"
author: "Rémi Taniel, Clara Maignan, Vincent Rosset, Hugo Delegue"
date: "24/09/2019"
output: 
  pdf_document:
    toc: true
---

\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1ère partie : Importation des données

On commence par importer les données grâce à :

```{r}
data <- read.table("/home/remi/Documents/Cours/AD/data/classe.csv", sep = ",", dec = ".", header = TRUE)
```

Puis on visualise les données grâce à la fonction str(...) :

```{r}
str(data)
```

Notre jeu de données comporte 14 obervations (14 réponses) et 4 variables (les 3 questions et un libellé), on remarque également que toutes les variables sont des variables qualitatives, c'est à dire, le sexe :

* H: Homme
* F: Femme

Les réponses à la seconde question (quelle est la couleur de tes cheveux) sont :

* BLO: Blond
* BRU: Brun
* NOIR: Noir
* CHA: Chatain

Puis pour la région de naissance :

* HDF: Haut De France:
* IDF: Ile de France
* PDLNORM: Pays de la Loire et Normandie
* SUD: Sud
* Autre: à l'étranger

On se décide de visualiser les 6 premières lignes de nos données ;

```{r}
head(data)
```

On remarque que nous devons enlever la colonne libellé, on décide donc de formater nos données pour donner un identifiant aux différentes lignes, dans notre cas, ce sera la variable libelle :

```{r}
rownames(data) <- data$libelle
data <- data[,-1]
head(data)
```

# 2e partie : Mise en oeuvre de l'ACM

Pour réaliser l'ACM, nous aurons besoin du package FactoMineR :

```{r}
library(FactoMineR)
```

Puis on range les résultats de l'ACM (valeurs propres, coordonnées, contribution) dans data.ca, dans notre cas, nous ne retenons que les 4 premiers axes et on souhaite que les graphiques ne soient pas générés lors de l'appel de la fonction :

```{r}
data.mca <- FactoMineR::MCA(data, ncp = 5, graph = FALSE)
```

(Explication de où se trouve les variables ?)

# 3e partie : Analyse des résultats

## Nombre d'axe à retenir

Pour connaître le nombre d'axe que nous devons retenir, nous pouvons utiliser 3 critères :

* Part d'inertie supérieure à la moyenne
* Part d'inertie cumulée supérieure à 80%
* Critère du coude

Pour rappel, les valeurs propres des différents axes sont stockés dans :

```{r}
data.mca$eig
```


### Part d'inertie supérieure à la moyenne

La moyenne des part d'inertie expliquée par chaque axe peut être obtenue par :

```{r}
mean(data.mca$eig[,2])
```

Selon ce critère, nous pouvons retenir les 4 premiers axes, qui possèdent tous une part d'inertie supérieure à la moyenne calculée qui est de 12.5.

### Part d'inertie cumulée supérieure à 80%

Tout comme le précédent critère, nous retenons les 5 premiers axes, en effet, ces 5 dimensions expliquent 87,36% de l'inertie totale portée par nos données.

### Critère du coude

Afin d'appliquer ce critère, nous devons dans un premier temps, tracer le graphique suivant :

```{r}
barplot(data.mca$eig[,2])
```

Le coude apparaît entre la 5e et 6e dimension, donc en utilisant ce critère nous devons également retenir 5 dimensions.

### Conclusion sur le nombre d'axe à retenir

Selon les 3 critères, nous devons seulement retenir les 4 premiers axes, on retrouve donc la valeur du paramètre nommé ncp lors de l'appel à la fonction qui calcule l'ACM (FactoMineR::MCA).

## Analyse des quatres premiers axes en fonction des modalités

Pour obtenir les données des 4 premiers axes en fonction des modalités, on utilisa l'information suivante :

```{r}
data.mca$var
```

Pour chacune des 4 dimensions, nous allons retenir les variables dont la contribution est supérieure à la moyenne (soit 8.33), puis pour chacune des marques retenues, nous allons noter la qualité de leur répresentation sous cet axe, ainsi que le signe de ses coordonnées.

### Dimension 1



### Dimension 2

| Mddalité | Contribution | Qualité | Signe |
| ----- | ---- | ---- | ---- |
| F | 17.4 | 0.583 | - |
| H | 17.4 | 0.583 | + |
| BLO | 16.1 | 0.378 | - |
| BRU | 14.8 | 0.435 | + |
| PDLNORM | 14.2 | 0.277 | - |
| Somme | 79.9 | 

Ces 5 modalités représentent 79.9% de l'information totale portée par la dimension 2, cet axe oppose les variables Homme (`H`) et Brund (`BRU`) aux variables Femmes (`F`), Blond (`BLO`) et Pays de la Loire Normandie (`PDLNORM`).

### Dimension 3



### Dimension 4



## Analyse des quatres premiers axes en fonction des individus

Nous allons reprendre la même méthode que précédemment mais en l'appliquant aux individus et non plus aux modalités, dans ce cas on utilisera les données suivantes :

```{r}
data.mca$ind
```

Comme précédemment, pour chaque dimension, 

### Dimension 1



### Dimension 2

| Individu | Contribution | Qualité | Signe |
| ----- | ---- | ---- | ---- |
| EL11 | 23.3 | 0.57 | + |
| EL1 | 18.7 | 0.46 | + |
| EL7 | 12 | 0.47 | - |
| EL12 | 12 | 0.47 | - |
| Somme | 66 |

Ces 4 individus expliquent 66% de l'information portée par l'axe 2. Il oppose l'EL11 et 1 aux élèves 7 et 12.
### Dimension 3



### Dimension 4



## Interprétation du premier plan factoriel

On peut obtenir les projections des marques et des attributs dans le premier plan factoriel grâce à :

```{r}
plot.MCA(data.mca, axes = c(1, 2))
```

l'axe2 oppose les hommes bruns représentés par les EL12 et 7 aux femmes blondes représentées par les EL11 et 1.